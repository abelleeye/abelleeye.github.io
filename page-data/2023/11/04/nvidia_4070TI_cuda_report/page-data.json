{"componentChunkName":"component---gatsby-theme-replica-src-templates-post-tsx","path":"/2023/11/04/nvidia_4070TI_cuda_report/","result":{"data":{"post":{"body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Nvidia 4070Ti cuda report\",\n  \"date\": \"2023-11-04T16:04:00.000Z\",\n  \"tags\": [\"Nvidia\"],\n  \"category\": \"Nvidia\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"asyncAPI\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[./asyncAPI] - Starting...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nCUDA device [NVIDIA GeForce RTX 4070 Ti]\\ntime spent executing by the GPU: 5.63\\ntime spent by CPU in CUDA calls: 2.81\\nCPU executed 48686 iterations while waiting for GPU to finish\\n\")), mdx(\"h1\", null, \"bandwidthTest\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[CUDA Bandwidth Test] - Starting...\\nRunning on...\\n\\n Device 0: NVIDIA GeForce RTX 4070 Ti\\n Quick Mode\\n\\n Host to Device Bandwidth, 1 Device(s)\\n PINNED Memory Transfers\\n   Transfer Size (Bytes)    Bandwidth(GB/s)\\n   32000000         24.0\\n\\n Device to Host Bandwidth, 1 Device(s)\\n PINNED Memory Transfers\\n   Transfer Size (Bytes)    Bandwidth(GB/s)\\n   32000000         26.3\\n\\n Device to Device Bandwidth, 1 Device(s)\\n PINNED Memory Transfers\\n   Transfer Size (Bytes)    Bandwidth(GB/s)\\n   32000000         457.7\\n\\nResult = PASS\\n\\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\\n\")), mdx(\"h1\", null, \"batchCUBLAS\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"batchCUBLAS Starting...\\n\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n\\n ==== Running single kernels ==== \\n\\nTesting sgemm\\n#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbf800000, -1) beta= (0x40000000, 2)\\n#### args: lda=128 ldb=128 ldc=128\\n^^^^ elapsed = 0.00437307 sec  GFLOPS=0.95912\\n@@@@ sgemm test OK\\nTesting dgemm\\n#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0x0000000000000000, 0) beta= (0x0000000000000000, 0)\\n#### args: lda=128 ldb=128 ldc=128\\n^^^^ elapsed = 0.00002599 sec  GFLOPS=161.396\\n@@@@ dgemm test OK\\n\\n ==== Running N=10 without streams ==== \\n\\nTesting sgemm\\n#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbf800000, -1) beta= (0x00000000, 0)\\n#### args: lda=128 ldb=128 ldc=128\\n^^^^ elapsed = 0.00005698 sec  GFLOPS=736.075\\n@@@@ sgemm test OK\\nTesting dgemm\\n#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbff0000000000000, -1) beta= (0x0000000000000000, 0)\\n#### args: lda=128 ldb=128 ldc=128\\n^^^^ elapsed = 0.00040507 sec  GFLOPS=103.544\\n@@@@ dgemm test OK\\n\\n ==== Running N=10 with streams ==== \\n\\nTesting sgemm\\n#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0x40000000, 2) beta= (0x40000000, 2)\\n#### args: lda=128 ldb=128 ldc=128\\n^^^^ elapsed = 0.00006294 sec  GFLOPS=666.371\\n@@@@ sgemm test OK\\nTesting dgemm\\n#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbff0000000000000, -1) beta= (0x0000000000000000, 0)\\n#### args: lda=128 ldb=128 ldc=128\\n^^^^ elapsed = 0.00009298 sec  GFLOPS=451.082\\n@@@@ dgemm test OK\\n\\n ==== Running N=10 batched ==== \\n\\nTesting sgemm\\n#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0x3f800000, 1) beta= (0xbf800000, -1)\\n#### args: lda=128 ldb=128 ldc=128\\n^^^^ elapsed = 0.00007200 sec  GFLOPS=582.523\\n@@@@ sgemm test OK\\nTesting dgemm\\n#### args: ta=0 tb=0 m=128 n=128 k=128  alpha = (0xbff0000000000000, -1) beta= (0x4000000000000000, 2)\\n#### args: lda=128 ldb=128 ldc=128\\n^^^^ elapsed = 0.00043511 sec  GFLOPS=96.3955\\n@@@@ dgemm test OK\\n\\nTest Summary\\n0 error(s)\\n\")), mdx(\"h1\", null, \"bf16TensorCoreGemm\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Initializing...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nM: 8192 (16 x 512)\\nN: 8192 (16 x 512)\\nK: 8192 (16 x 512)\\nPreparing data for GPU...\\nRequired shared memory size: 72 Kb\\nComputing using high performance kernel = 0 - compute_bf16gemm_async_copy\\nTime: 20.165665 ms\\nTFLOPS: 54.52\\n\")), mdx(\"h1\", null, \"binaryPartitionCG\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n\\nLaunching 120 blocks with 768 threads...\\n\\nArray size = 102400 Num of Odds = 50945 Sum of Odds = 1272565 Sum of Evens 1233938\\n\\n...Done.\\n\\n\")), mdx(\"h1\", null, \"binomialOptions\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[./binomialOptions] - Starting...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nGenerating input data...\\nRunning GPU binomial tree...\\nOptions count            : 1024     \\nTime steps               : 2048     \\nbinomialOptionsGPU() time: 0.640000 msec\\nOptions per second       : 1600000.035763     \\nRunning CPU binomial tree...\\nComparing the results...\\nGPU binomial vs. Black-Scholes\\nL1 norm: 2.220214E-04\\nCPU binomial vs. Black-Scholes\\nL1 norm: 2.220922E-04\\nCPU binomial vs. GPU binomial\\nL1 norm: 7.997008E-07\\nShutting down...\\n\\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\\n\\nTest passed\\n\")), mdx(\"h1\", null, \"binomialOptions_nvrtc\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[./binomialOptions_nvrtc] - Starting...\\nGenerating input data...\\nRunning GPU binomial tree...\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> GPU Device has SM 8.9 compute capability\\nOptions count            : 1024     \\nTime steps               : 2048     \\nbinomialOptionsGPU() time: 142.667007 msec\\nOptions per second       : 7177.552949     \\nRunning CPU binomial tree...\\nComparing the results...\\nGPU binomial vs. Black-Scholes\\nL1 norm: 2.216577E-04\\nCPU binomial vs. Black-Scholes\\nL1 norm: 9.435265E-05\\nCPU binomial vs. GPU binomial\\nL1 norm: 1.513570E-04\\nShutting down...\\n\\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\\n\\nTest passed\\n\")), mdx(\"h1\", null, \"BlackScholes\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[./BlackScholes] - Starting...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nInitializing data...\\n...allocating CPU memory for options.\\n...allocating GPU memory for options.\\n...generating input data in CPU mem.\\n...copying input data to GPU mem.\\nData init done.\\n\\nExecuting Black-Scholes GPU kernel (512 iterations)...\\nOptions count             : 8000000     \\nBlackScholesGPU() time    : 0.180459 msec\\nEffective memory bandwidth: 443.314048 GB/s\\nGigaoptions per second    : 44.331405     \\n\\nBlackScholes, Throughput = 44.3314 GOptions/s, Time = 0.00018 s, Size = 8000000 options, NumDevsUsed = 1, Workgroup = 128\\n\\nReading back GPU results...\\nChecking the results...\\n...running CPU calculations.\\n\\nComparing the results...\\nL1 norm: 1.741792E-07\\nMax absolute error: 1.192093E-05\\n\\nShutting down...\\n...releasing GPU memory.\\n...releasing CPU memory.\\nShutdown done.\\n\\n[BlackScholes] - Test Summary\\n\\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\\n\\nTest passed\\n\")), mdx(\"h1\", null, \"BlackScholes_nvrtc\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[./BlackScholes_nvrtc] - Starting...\\nInitializing data...\\n...allocating CPU memory for options.\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> GPU Device has SM 8.9 compute capability\\n...allocating GPU memory for options.\\n...generating input data in CPU mem.\\n...copying input data to GPU mem.\\nData init done.\\n\\nExecuting Black-Scholes GPU kernel (512 iterations)...\\nOptions count             : 8000000     \\nBlackScholesGPU() time    : 0.180770 msec\\nEffective memory bandwidth: 442.552452 GB/s\\nGigaoptions per second    : 44.255245     \\n\\nBlackScholes, Throughput = 44.2552 GOptions/s, Time = 0.00018 s, Size = 8000000 options, NumDevsUsed = 1, Workgroup = 128\\n\\nReading back GPU results...\\nChecking the results...\\n...running CPU calculations.\\n\\nComparing the results...\\nL1 norm: 1.741792E-07\\nMax absolute error: 1.192093E-05\\n\\nShutting down...\\n...releasing GPU memory.\\n...releasing CPU memory.\\nShutdown done.\\n\\n[./BlackScholes_nvrtc] - Test Summary\\nTest passed\\n\")), mdx(\"h1\", null, \"c++11_cuda\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nRead 3223503 byte corpus from ../../../../Samples/0_Introduction/c++11_cuda/warandpeace.txt\\ncounted 107310 instances of 'x', 'y', 'z', or 'w' in \\\"../../../../Samples/0_Introduction/c++11_cuda/warandpeace.txt\\\"\\n\")), mdx(\"h1\", null, \"cdpAdvancedQuicksort\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nGPU device NVIDIA GeForce RTX 4070 Ti has compute capabilities (SM 8.9)\\nRunning qsort on 1000000 elements with seed 0, on NVIDIA GeForce RTX 4070 Ti\\n    cdpAdvancedQuicksort PASSED\\nSorted 1000000 elems in 5.871 ms (170.341 Melems/sec)\\n\")), mdx(\"h1\", null, \"cdpBezierTessellation\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Running on GPU 0 (NVIDIA GeForce RTX 4070 Ti)\\nComputing Bezier Lines (CUDA Dynamic Parallelism Version) ... Done!\\n\")), mdx(\"h1\", null, \"cdpQuadtree\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nGPU device NVIDIA GeForce RTX 4070 Ti has compute capabilities (SM 8.9)\\nLaunching CDP kernel to build the quadtree\\nResults: OK\\n\")), mdx(\"h1\", null, \"cdpSimplePrint\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"starting Simple Print (CUDA Dynamic Parallelism)\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n***************************************************************************\\nThe CPU launches 2 blocks of 2 threads each. On the device each thread will\\nlaunch 2 blocks of 2 threads each. The GPU we will do that recursively\\nuntil it reaches max_depth=2\\n\\nIn total 2+8=10 blocks are launched!!! (8 from the GPU)\\n***************************************************************************\\n\\nLaunching cdp_kernel() with CUDA Dynamic Parallelism:\\n\\nBLOCK 0 launched by the host\\nBLOCK 1 launched by the host\\n|  BLOCK 2 launched by thread 0 of block 0\\n|  BLOCK 4 launched by thread 0 of block 1\\n|  BLOCK 3 launched by thread 0 of block 0\\n|  BLOCK 5 launched by thread 0 of block 1\\n|  BLOCK 6 launched by thread 1 of block 0\\n|  BLOCK 7 launched by thread 1 of block 0\\n|  BLOCK 8 launched by thread 1 of block 1\\n|  BLOCK 9 launched by thread 1 of block 1\\n\")), mdx(\"h1\", null, \"cdpSimpleQuicksort\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nInitializing data:\\nRunning quicksort on 128 elements\\nLaunching kernel on the GPU\\nValidating results: OK\\n\")), mdx(\"h1\", null, \"clock\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"CUDA Clock sample\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nAverage clocks/block = 2276.953125\\n\")), mdx(\"h1\", null, \"clock_nvrtc\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"CUDA Clock sample\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> GPU Device has SM 8.9 compute capability\\nAverage clocks/block = 2316.890625\\n\")), mdx(\"h1\", null, \"concurrentKernels\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[./concurrentKernels] - Starting...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n> Detected Compute SM 8.9 hardware with 60 multi-processors\\nExpected time for serial execution of 8 kernels = 0.080s\\nExpected time for concurrent execution of 8 kernels = 0.010s\\nMeasured time for sample = 0.010s\\nTest passed\\n\")), mdx(\"h1\", null, \"conjugateGradientMultiDeviceCG\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Starting [conjugateGradientMultiDeviceCG]...\\nGPU Device 0: \\\"NVIDIA GeForce RTX 4070 Ti\\\" with compute capability 8.9\\nNo two or more GPUs with same architecture capable of concurrentManagedAccess found. \\nWaiving the sample\\n\")), mdx(\"h1\", null, \"convolutionFFT2D\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[./convolutionFFT2D] - Starting...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nTesting built-in R2C / C2R FFT-based convolution\\n...allocating memory\\n...generating random input data\\n...creating R2C & C2R FFT plans for 2048 x 2048\\n...uploading to GPU and padding convolution kernel and input data\\n...transforming convolution kernel\\n...running GPU FFT convolution: 20100.502416 MPix/s (0.199000 ms)\\n...reading back GPU convolution results\\n...running reference CPU convolution\\n...comparing the results: rel L2 = 9.395370E-08 (max delta = 1.208283E-06)\\nL2norm Error OK\\n...shutting down\\nTesting custom R2C / C2R FFT-based convolution\\n...allocating memory\\n...generating random input data\\n...creating C2C FFT plan for 2048 x 1024\\n...uploading to GPU and padding convolution kernel and input data\\n...transforming convolution kernel\\n...running GPU FFT convolution: 14760.147718 MPix/s (0.271000 ms)\\n...reading back GPU FFT results\\n...running reference CPU convolution\\n...comparing the results: rel L2 = 1.067915E-07 (max delta = 9.817303E-07)\\nL2norm Error OK\\n...shutting down\\nTesting updated custom R2C / C2R FFT-based convolution\\n...allocating memory\\n...generating random input data\\n...creating C2C FFT plan for 2048 x 1024\\n...uploading to GPU and padding convolution kernel and input data\\n...transforming convolution kernel\\n...running GPU FFT convolution: 25477.706155 MPix/s (0.157000 ms)\\n...reading back GPU FFT results\\n...running reference CPU convolution\\n...comparing the results: rel L2 = 1.065127E-07 (max delta = 9.817303E-07)\\nL2norm Error OK\\n...shutting down\\nTest Summary: 0 errors\\nTest passed\\n\")), mdx(\"h1\", null, \"cppIntegration\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nHello World.\\nHello World.\\n\")), mdx(\"h1\", null, \"cppOverload\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"C++ Function Overloading starting...\\nDevice Count: 1\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nShared Size:   1024\\nConstant Size: 0\\nLocal Size:    0\\nMax Threads Per Block: 1024\\nNumber of Registers: 12\\nPTX Version: 89\\nBinary Version: 89\\nsimple_kernel(const int *pIn, int *pOut, int a) PASSED\\n\\nShared Size:   2048\\nConstant Size: 0\\nLocal Size:    0\\nMax Threads Per Block: 1024\\nNumber of Registers: 12\\nPTX Version: 89\\nBinary Version: 89\\nsimple_kernel(const int2 *pIn, int *pOut, int a) PASSED\\n\\nShared Size:   2048\\nConstant Size: 0\\nLocal Size:    0\\nMax Threads Per Block: 1024\\nNumber of Registers: 16\\nPTX Version: 89\\nBinary Version: 89\\nsimple_kernel(const int *pIn1, const int *pIn2, int *pOut, int a) PASSED\\n\\n\")), mdx(\"h1\", null, \"cudaCompressibleMemory\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nGeneric memory compression support is available\\nRunning saxpy on 167772160 bytes of Compressible memory\\nRunning saxpy with 120 blocks x 768 threads = 0.188 ms 2.671 TB/s\\nRunning saxpy on 167772160 bytes of Non-Compressible memory\\nRunning saxpy with 120 blocks x 768 threads = 1.164 ms 0.432 TB/s\\n\\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\\n\")), mdx(\"h1\", null, \"cudaOpenMP\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./cudaOpenMP Starting...\\n\\nnumber of host CPUs:    20\\nnumber of CUDA devices: 1\\n   0: NVIDIA GeForce RTX 4070 Ti\\n---------------------------\\nCPU thread 0 (of 1) uses CUDA device 0\\n---------------------------\\n\")), mdx(\"h1\", null, \"cudaTensorCoreGemm\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Initializing...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nM: 4096 (16 x 256)\\nN: 4096 (16 x 256)\\nK: 4096 (16 x 256)\\nPreparing data for GPU...\\nRequired shared memory size: 64 Kb\\nComputing... using high performance kernel compute_gemm \\nTime: 2.207872 ms\\nTFLOPS: 62.25\\n\")), mdx(\"h1\", null, \"deviceQuery\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./deviceQuery Starting...\\n\\n CUDA Device Query (Runtime API) version (CUDART static linking)\\n\\nDetected 1 CUDA Capable device(s)\\n\\nDevice 0: \\\"NVIDIA GeForce RTX 4070 Ti\\\"\\n  CUDA Driver Version / Runtime Version          12.2 / 12.1\\n  CUDA Capability Major/Minor version number:    8.9\\n  Total amount of global memory:                 11976 MBytes (12557942784 bytes)\\n  (060) Multiprocessors, (128) CUDA Cores/MP:    7680 CUDA Cores\\n  GPU Max Clock rate:                            2730 MHz (2.73 GHz)\\n  Memory Clock rate:                             10501 Mhz\\n  Memory Bus Width:                              192-bit\\n  L2 Cache Size:                                 50331648 bytes\\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\\n  Total amount of constant memory:               65536 bytes\\n  Total amount of shared memory per block:       49152 bytes\\n  Total shared memory per multiprocessor:        102400 bytes\\n  Total number of registers available per block: 65536\\n  Warp size:                                     32\\n  Maximum number of threads per multiprocessor:  1536\\n  Maximum number of threads per block:           1024\\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\\n  Maximum memory pitch:                          2147483647 bytes\\n  Texture alignment:                             512 bytes\\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\\n  Run time limit on kernels:                     Yes\\n  Integrated GPU sharing Host Memory:            No\\n  Support host page-locked memory mapping:       Yes\\n  Alignment requirement for Surfaces:            Yes\\n  Device has ECC support:                        Disabled\\n  Device supports Unified Addressing (UVA):      Yes\\n  Device supports Managed Memory:                Yes\\n  Device supports Compute Preemption:            Yes\\n  Supports Cooperative Kernel Launch:            Yes\\n  Supports MultiDevice Co-op Kernel Launch:      Yes\\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\\n  Compute Mode:\\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\\n\\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.2, CUDA Runtime Version = 12.1, NumDevs = 1\\nResult = PASS\\n\")), mdx(\"h1\", null, \"deviceQueryDrv\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./deviceQueryDrv Starting...\\n\\nCUDA Device Query (Driver API) statically linked version \\nDetected 1 CUDA Capable device(s)\\n\\nDevice 0: \\\"NVIDIA GeForce RTX 4070 Ti\\\"\\n  CUDA Driver Version:                           12.2\\n  CUDA Capability Major/Minor version number:    8.9\\n  Total amount of global memory:                 11976 MBytes (12557942784 bytes)\\n  (60) Multiprocessors, (128) CUDA Cores/MP:     7680 CUDA Cores\\n  GPU Max Clock rate:                            2730 MHz (2.73 GHz)\\n  Memory Clock rate:                             10501 Mhz\\n  Memory Bus Width:                              192-bit\\n  L2 Cache Size:                                 50331648 bytes\\n  Max Texture Dimension Sizes                    1D=(131072) 2D=(131072, 65536) 3D=(16384, 16384, 16384)\\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\\n  Total amount of constant memory:               65536 bytes\\n  Total amount of shared memory per block:       49152 bytes\\n  Total number of registers available per block: 65536\\n  Warp size:                                     32\\n  Maximum number of threads per multiprocessor:  1536\\n  Maximum number of threads per block:           1024\\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\\n  Max dimension size of a grid size (x,y,z):    (2147483647, 65535, 65535)\\n  Texture alignment:                             512 bytes\\n  Maximum memory pitch:                          2147483647 bytes\\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\\n  Run time limit on kernels:                     Yes\\n  Integrated GPU sharing Host Memory:            No\\n  Support host page-locked memory mapping:       Yes\\n  Concurrent kernel execution:                   Yes\\n  Alignment requirement for Surfaces:            Yes\\n  Device has ECC support:                        Disabled\\n  Device supports Unified Addressing (UVA):      Yes\\n  Device supports Managed Memory:                Yes\\n  Device supports Compute Preemption:            Yes\\n  Supports Cooperative Kernel Launch:            Yes\\n  Supports MultiDevice Co-op Kernel Launch:      Yes\\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\\n  Compute Mode:\\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\\nResult = PASS\\n\")), mdx(\"h1\", null, \"dmmaTensorCoreGemm\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Initializing...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nM: 8192 (8 x 1024)\\nN: 8192 (8 x 1024)\\nK: 4096 (4 x 1024)\\nPreparing data for GPU...\\nRequired shared memory size: 68 Kb\\nComputing using high performance kernel = 0 - compute_dgemm_async_copy\\nTime: 942.316528 ms\\nFP64 TFLOPS: 0.58\\n\")), mdx(\"h1\", null, \"dwtHaar1D\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./dwtHaar1D Starting...\\n\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nsource file    = \\\"../../../../Samples/5_Domain_Specific/dwtHaar1D/data/signal.dat\\\"\\nreference file = \\\"result.dat\\\"\\ngold file      = \\\"../../../../Samples/5_Domain_Specific/dwtHaar1D/data/regression.gold.dat\\\"\\nReading signal from \\\"../../../../Samples/5_Domain_Specific/dwtHaar1D/data/signal.dat\\\"\\nWriting result to \\\"result.dat\\\"\\nReading reference result from \\\"../../../../Samples/5_Domain_Specific/dwtHaar1D/data/regression.gold.dat\\\"\\nTest success!\\n\")), mdx(\"h1\", null, \"dxtc\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./dxtc Starting...\\n\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nImage Loaded '../../../../Samples/5_Domain_Specific/dxtc/data/teapot512_std.ppm', 512 x 512 pixels\\n\\nRunning DXT Compression on 512 x 512 image...\\n\\n16384 Blocks, 64 Threads per Block, 1048576 Threads in Grid...\\n\\ndxtc, Throughput = 530.6559 MPixels/s, Time = 0.00049 s, Size = 262144 Pixels, NumDevsUsed = 1, Workgroup = 64\\n\\nChecking accuracy...\\nRMS(reference, result) = 0.000000\\n\\nTest passed\\n\")), mdx(\"h1\", null, \"encode_output\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./markdown_echo_for.sh: line 10: ./encode_output: Is a directory\\n\")), mdx(\"h1\", null, \"fastWalshTransform\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./fastWalshTransform Starting...\\n\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nInitializing data...\\n...allocating CPU memory\\n...allocating GPU memory\\n...generating data\\nData length: 8388608; kernel length: 128\\nRunning GPU dyadic convolution using Fast Walsh Transform...\\nGPU time: 1.171000 ms; GOP/s: 247.145154\\nReading back GPU results...\\nRunning straightforward CPU dyadic convolution...\\nComparing the results...\\nShutting down...\\nL2 norm: 1.021579E-07\\nTest passed\\n\")), mdx(\"h1\", null, \"FDTD3d\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./FDTD3d Starting...\\n\\nSet-up, based upon target device GMEM size...\\n getTargetDeviceGlobalMemSize\\n cudaGetDeviceCount\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n cudaGetDeviceProperties\\n generateRandomData\\n\\nFDTD on 376 x 376 x 376 volume with symmetric filter radius 4 for 5 timesteps...\\n\\nfdtdReference...\\n calloc intermediate\\n Host FDTD loop\\n    t = 0\\n    t = 1\\n    t = 2\\n    t = 3\\n    t = 4\\n\\nfdtdReference complete\\nfdtdGPU...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n set block size to 32x16\\n set grid size to 12x24\\n GPU FDTD loop\\n    t = 0 launch kernel\\n    t = 1 launch kernel\\n    t = 2 launch kernel\\n    t = 3 launch kernel\\n    t = 4 launch kernel\\n\\nfdtdGPU complete\\n\\nCompareData (tolerance 0.000100)...\\n\")), mdx(\"h1\", null, \"fp16ScalarProduct\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nResult native operators : 658990.000000 \\nResult intrinsics   : 658990.000000 \\n&&&& fp16ScalarProduct PASSED\\n\")), mdx(\"h1\", null, \"globalToShmemAsyncCopy\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[globalToShmemAsyncCopy] - Starting...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nMatrixA(1280,1280), MatrixB(1280,1280)\\nRunning kernel = 0 - AsyncCopyMultiStageLargeChunk\\nComputing result using CUDA Kernel...\\ndone\\nPerformance= 2897.51 GFlop/s, Time= 1.448 msec, Size= 4194304000 Ops, WorkgroupSize= 256 threads/block\\nChecking computed result for correctness: Result = PASS\\n\\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\\n\")), mdx(\"h1\", null, \"graphMemoryFootprint\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nDriver version is: 12.2\\nRunning sample.\\n================================\\nRunning virtual address reuse example.\\nSequential allocations & frees within a single graph enable CUDA to reuse virtual addresses.\\n\\nCheck confirms that d_a and d_b share a virtual address.\\n    FOOTPRINT: 67108864 bytes\\n\\nCleaning up example by trimming device memory.\\n    FOOTPRINT: 0 bytes\\n\\n================================\\nRunning physical memory reuse example.\\nCUDA reuses the same physical memory for allocations from separate graphs when the allocation lifetimes don't overlap.\\n\\nCreating the graph execs does not reserve any physical memory.\\n    FOOTPRINT: 0 bytes\\n\\nThe first graph launched reserves the memory it needs.\\n    FOOTPRINT: 67108864 bytes\\nA subsequent launch of the same graph in the same stream reuses the same physical memory. Thus the memory footprint does not grow here.\\n    FOOTPRINT: 67108864 bytes\\n\\nSubsequent launches of other graphs in the same stream also reuse the physical memory. Thus the memory footprint does not grow here.\\n01:     FOOTPRINT: 67108864 bytes\\n02:     FOOTPRINT: 67108864 bytes\\n03:     FOOTPRINT: 67108864 bytes\\n04:     FOOTPRINT: 67108864 bytes\\n05:     FOOTPRINT: 67108864 bytes\\n06:     FOOTPRINT: 67108864 bytes\\n07:     FOOTPRINT: 67108864 bytes\\n\\nCheck confirms all graphs use a different virtual address.\\n\\nCleaning up example by trimming device memory.\\n    FOOTPRINT: 0 bytes\\n\\n================================\\nRunning simultaneous streams example.\\nGraphs that can run concurrently need separate physical memory. In this example, each graph launched in a separate stream increases the total memory footprint.\\n\\nWhen launching a new graph, CUDA may reuse physical memory from a graph whose execution has already finished -- even if the new graph is being launched in a different stream from the completed graph. Therefore, a kernel node is added to the graphs to increase runtime.\\n\\nInitial footprint:\\n    FOOTPRINT: 0 bytes\\n\\nEach graph launch in a seperate stream grows the memory footprint:\\n01:     FOOTPRINT: 67108864 bytes\\n02:     FOOTPRINT: 134217728 bytes\\n03:     FOOTPRINT: 201326592 bytes\\n04:     FOOTPRINT: 268435456 bytes\\n05:     FOOTPRINT: 335544320 bytes\\n06:     FOOTPRINT: 402653184 bytes\\n07:     FOOTPRINT: 469762048 bytes\\n\\nCleaning up example by trimming device memory.\\n    FOOTPRINT: 0 bytes\\n\\n================================\\nRunning unfreed streams example.\\nCUDA cannot reuse phyiscal memory from graphs which do not free their allocations.\\n\\nDespite being launched in the same stream, each graph launch grows the memory footprint. Since the allocation is not freed, CUDA keeps the memory valid for use.\\n00:     FOOTPRINT: 67108864 bytes\\n01:     FOOTPRINT: 134217728 bytes\\n02:     FOOTPRINT: 201326592 bytes\\n03:     FOOTPRINT: 268435456 bytes\\n04:     FOOTPRINT: 335544320 bytes\\n05:     FOOTPRINT: 402653184 bytes\\n06:     FOOTPRINT: 469762048 bytes\\n07:     FOOTPRINT: 536870912 bytes\\n\\nTrimming does not impact the memory footprint since the un-freed allocations are still holding onto the memory.\\n    FOOTPRINT: 536870912 bytes\\n\\nFreeing the allocations does not shrink the footprint.\\n    FOOTPRINT: 536870912 bytes\\n\\nSince the allocations are now freed, trimming does reduce the footprint even when the graph execs are not yet destroyed.\\n    FOOTPRINT: 0 bytes\\n\\nCleaning up example by trimming device memory.\\n    FOOTPRINT: 0 bytes\\n\\n================================\\nSample complete.\\n\")), mdx(\"h1\", null, \"graphMemoryNodes\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nDriver version is: 12.2\\nSetting up sample.\\nSetup complete.\\n\\nRunning negateSquares in a stream.\\nValidating negateSquares in a stream...\\nValidation PASSED!\\n\\nRunning negateSquares in a stream-captured graph.\\nValidating negateSquares in a stream-captured graph...\\nValidation PASSED!\\n\\nRunning negateSquares in an explicitly constructed graph.\\nCheck verified that d_negSquare and d_input share a virtual address.\\nValidating negateSquares in an explicitly constructed graph...\\nValidation PASSED!\\n\\nRunning negateSquares with d_negSquare freed outside the stream.\\nCheck verified that d_negSquare and d_input share a virtual address.\\nValidating negateSquares with d_negSquare freed outside the stream...\\nValidation PASSED!\\n\\nRunning negateSquares with d_negSquare freed outside the graph.\\nValidating negateSquares with d_negSquare freed outside the graph...\\nValidation PASSED!\\n\\nRunning negateSquares with d_negSquare freed in a different graph.\\nValidating negateSquares with d_negSquare freed in a different graph...\\nValidation PASSED!\\n\\nCleaning up sample.\\nCleanup complete. Exiting sample.\\n\")), mdx(\"h1\", null, \"HSOpticalFlow\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"HSOpticalFlow Starting...\\n\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nLoading \\\"frame10.ppm\\\" ...\\nLoading \\\"frame11.ppm\\\" ...\\nComputing optical flow on CPU...\\nComputing optical flow on GPU...\\nL1 error : 0.044308\\n\")), mdx(\"h1\", null, \"immaTensorCoreGemm\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Initializing...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nM: 4096 (16 x 256)\\nN: 4096 (16 x 256)\\nK: 4096 (16 x 256)\\nPreparing data for GPU...\\nRequired shared memory size: 64 Kb\\nComputing... using high performance kernel compute_gemm_imma \\nTime: 0.983040 ms\\nTOPS: 139.81\\n\")), mdx(\"h1\", null, \"jacobiCudaGraphs\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nCPU iterations : 2954\\nCPU error : 4.988e-03\\nCPU Processing time: 1204.943970 (ms)\\nGPU iterations : 2954\\nGPU error : 4.988e-03\\nGPU Processing time: 63.724998 (ms)\\n&&&& jacobiCudaGraphs PASSED\\n\")), mdx(\"h1\", null, \"matrixMul\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[Matrix Multiply Using CUDA] - Starting...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nMatrixA(320,320), MatrixB(640,320)\\nComputing result using CUDA Kernel...\\ndone\\nPerformance= 2698.52 GFlop/s, Time= 0.049 msec, Size= 131072000 Ops, WorkgroupSize= 1024 threads/block\\nChecking computed result for correctness: Result = PASS\\n\\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\\n\")), mdx(\"h1\", null, \"matrixMulDrv\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[ matrixMulDrv (Driver API) ]\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> GPU Device has SM 8.9 compute capability\\n  Total amount of global memory:     12557942784 bytes\\n> findModulePath found file at <./matrixMul_kernel64.fatbin>\\n> initCUDA loading module: <./matrixMul_kernel64.fatbin>\\n> 16 block size selected\\nProcessing time: 0.023000 (ms)\\nChecking computed result for correctness: Result = PASS\\n\\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\\n\")), mdx(\"h1\", null, \"matrixMulDynlinkJIT\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[ matrixMulDynlinkJIT (CUDA dynamic linking) ]\\n> Device 0: \\\"NVIDIA GeForce RTX 4070 Ti\\\" with Compute 8.9 capability\\n> Compiling CUDA module\\n> PTX JIT log:\\n\\nTest run success!\\n\")), mdx(\"h1\", null, \"matrixMul_nvrtc\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[Matrix Multiply Using CUDA] - Starting...\\nMatrixA(320,320), MatrixB(640,320)\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> GPU Device has SM 8.9 compute capability\\nComputing result using CUDA Kernel...\\nChecking computed result for correctness: Result = PASS\\n\\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\\n\")), mdx(\"h1\", null, \"memMapIPCDrv\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"> findModulePath found file at <./memMapIpc_kernel64.ptx>\\n> initCUDA loading module: <./memMapIpc_kernel64.ptx>\\n> PTX JIT log:\\n\\nStep 0 done\\nProcess 0: verifying...\\n\")), mdx(\"h1\", null, \"mergeSort\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./mergeSort Starting...\\n\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nAllocating and initializing host arrays...\\n\\nAllocating and initializing CUDA arrays...\\n\\nInitializing GPU merge sort...\\nRunning GPU merge sort...\\nTime: 2.122000 ms\\nReading back GPU merge sort results...\\nInspecting the results...\\n...inspecting keys array: OK\\n...inspecting keys and values array: OK\\n...stability property: stable!\\nShutting down...\\n\")), mdx(\"h1\", null, \"MonteCarloMultiGPU\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./MonteCarloMultiGPU Starting...\\n\\nUsing single CPU thread for multiple GPUs\\nMonteCarloMultiGPU\\n==================\\nParallelization method  = streamed\\nProblem scaling         = weak\\nNumber of GPUs          = 1\\nTotal number of options = 8192\\nNumber of paths         = 262144\\nmain(): generating input data...\\nmain(): starting 1 host threads...\\nmain(): GPU statistics, streamed\\nGPU Device #0: NVIDIA GeForce RTX 4070 Ti\\nOptions         : 8192\\nSimulation paths: 262144\\n\\nTotal time (ms.): 6.029000\\n    Note: This is elapsed time for all to compute.\\nOptions per sec.: 1358766.008351\\nmain(): comparing Monte Carlo and Black-Scholes results...\\nShutting down...\\nTest Summary...\\nL1 norm        : 4.898407E-04\\nAverage reserve: 12.983048\\n\\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\\n\\nTest passed\\n\")), mdx(\"h1\", null, \"newdelete\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"newdelete Starting...\\n\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n > Container = Vector test OK\\n\\n > Container = Vector, using placement new on SMEM buffer test OK\\n\\n > Container = Vector, with user defined datatype test OK\\n\\nTest Summary: 3/3 succesfully run\\n\")), mdx(\"h1\", null, \"NV12toBGRandResize\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n\\nTEST#1:\\n  CUDA resize nv12(1920x1080 --> 640x480), batch: 24, average time: 0.036 ms ==> 0.001 ms/frame\\n  CUDA convert nv12(640x480) to bgr(640x480), batch: 24, average time: 0.230 ms ==> 0.010 ms/frame\\n\\nTEST#2:\\n  CUDA convert nv12(1920x1080) to bgr(1920x1080), batch: 24, average time: 1.630 ms ==> 0.068 ms/frame\\n  CUDA resize bgr(1920x1080 --> 640x480), batch: 24, average time: 1.223 ms ==> 0.051 ms/frame\\n\")), mdx(\"h1\", null, \"output\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./markdown_echo_for.sh: line 10: ./output: Is a directory\\n\")), mdx(\"h1\", null, \"p2pBandwidthLatencyTest\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]\\nDevice: 0, NVIDIA GeForce RTX 4070 Ti, pciBusID: 1, pciDeviceID: 0, pciDomainID:0\\n\\n***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\\nSo you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.\\n\\nP2P Connectivity Matrix\\n     D\\\\D     0\\n     0       1\\nUnidirectional P2P=Disabled Bandwidth Matrix (GB/s)\\n   D\\\\D     0 \\n     0 428.20 \\nUnidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\\n   D\\\\D     0 \\n     0 389.94 \\nBidirectional P2P=Disabled Bandwidth Matrix (GB/s)\\n   D\\\\D     0 \\n     0 387.46 \\nBidirectional P2P=Enabled Bandwidth Matrix (GB/s)\\n   D\\\\D     0 \\n     0 390.17 \\nP2P=Disabled Latency Matrix (us)\\n   GPU     0 \\n     0   1.20 \\n\\n   CPU     0 \\n     0   1.11 \\nP2P=Enabled Latency (P2P Writes) Matrix (us)\\n   GPU     0 \\n     0   1.16 \\n\\n   CPU     0 \\n     0   1.08 \\n\\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\\n\")), mdx(\"h1\", null, \"ptxjit\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[PTX Just In Time (JIT) Compilation (no-qatest)] - Starting...\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> findModulePath <./ptxjit_kernel64.ptx>\\n> initCUDA loading module: <./ptxjit_kernel64.ptx>\\nLoading ptxjit_kernel[] program\\nCUDA Link Completed in 0.000000ms. Linker Output:\\nptxas info    : 0 bytes gmem\\nptxas info    : Compiling entry function 'myKernel' for 'sm_89'\\nptxas info    : Function properties for myKernel\\nptxas         .     0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\\nptxas info    : Used 8 registers, 360 bytes cmem[0]\\ninfo    : 0 bytes gmem\\ninfo    : Function properties for 'myKernel':\\ninfo    : used 8 registers, 0 stack, 0 bytes smem, 360 bytes cmem[0], 0 bytes lmem\\nCUDA kernel launched\\n\")), mdx(\"h1\", null, \"quasirandomGenerator\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./quasirandomGenerator Starting...\\n\\nAllocating GPU memory...\\nAllocating CPU memory...\\nInitializing QRNG tables...\\n\\nTesting QRNG...\\n\\nquasirandomGenerator, Throughput = 59.8047 GNumbers/s, Time = 0.00005 s, Size = 3145728 Numbers, NumDevsUsed = 1, Workgroup = 384\\n\\nReading GPU results...\\nComparing to the CPU results...\\n\\nL1 norm: 7.275964E-12\\n\\nTesting inverseCNDgpu()...\\n\\nquasirandomGenerator-inverse, Throughput = 186.6901 GNumbers/s, Time = 0.00002 s, Size = 3145728 Numbers, NumDevsUsed = 1, Workgroup = 128\\nReading GPU results...\\n\\nComparing to the CPU results...\\nL1 norm: 9.439909E-08\\n\\nShutting down...\\n\")), mdx(\"h1\", null, \"quasirandomGenerator_nvrtc\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./quasirandomGenerator_nvrtc Starting...\\n\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> GPU Device has SM 8.9 compute capability\\nAllocating GPU memory...\\nAllocating CPU memory...\\nInitializing QRNG tables...\\n\\nTesting QRNG...\\n\\nquasirandomGenerator, Throughput = 57.5614 GNumbers/s, Time = 0.00005 s, Size = 3145728 Numbers, NumDevsUsed = 1, Workgroup = 384\\n\\nReading GPU results...\\nComparing to the CPU results...\\n\\nL1 norm: 7.275964E-12\\n\\nTesting inverseCNDgpu()...\\n\\nquasirandomGenerator-inverse, Throughput = 162.9911 GNumbers/s, Time = 0.00002 s, Size = 3145728 Numbers, NumDevsUsed = 1, Workgroup = 128\\nReading GPU results...\\n\\nComparing to the CPU results...\\nL1 norm: 9.439909E-08\\n\\nShutting down...\\n\")), mdx(\"h1\", null, \"simpleAssert\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"simpleAssert.cu:63: void testKernel(int): block: [1,0,0], thread: [28,0,0] Assertion `gtid < N` failed.\\nsimpleAssert.cu:63: void testKernel(int): block: [1,0,0], thread: [29,0,0] Assertion `gtid < N` failed.\\nsimpleAssert.cu:63: void testKernel(int): block: [1,0,0], thread: [30,0,0] Assertion `gtid < N` failed.\\nsimpleAssert.cu:63: void testKernel(int): block: [1,0,0], thread: [31,0,0] Assertion `gtid < N` failed.\\nsimpleAssert starting...\\nOS_System_Type.release = 5.15.0-82-generic\\nOS Info: <#91~20.04.1-Ubuntu SMP Fri Aug 18 16:24:39 UTC 2023>\\n\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nLaunch kernel to generate assertion failures\\n\\n-- Begin assert output\\n\\n\\n-- End assert output\\n\\nDevice assert failed as expected, CUDA error message is: device-side assert triggered\\n\\nsimpleAssert completed, returned OK\\n\")), mdx(\"h1\", null, \"simpleAssert_nvrtc\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"../../../../Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_kernel.cu:37: void testKernel(int): block: [1,0,0], thread: [28,0,0] Assertion `gtid < N` failed.\\n../../../../Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_kernel.cu:37: void testKernel(int): block: [1,0,0], thread: [29,0,0] Assertion `gtid < N` failed.\\n../../../../Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_kernel.cu:37: void testKernel(int): block: [1,0,0], thread: [30,0,0] Assertion `gtid < N` failed.\\n../../../../Samples/0_Introduction/simpleAssert_nvrtc/simpleAssert_kernel.cu:37: void testKernel(int): block: [1,0,0], thread: [31,0,0] Assertion `gtid < N` failed.\\nsimpleAssert_nvrtc starting...\\nLaunch kernel to generate assertion failures\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> GPU Device has SM 8.9 compute capability\\n\\n-- Begin assert output\\n\\n\\n-- End assert output\\n\\nDevice assert failed as expected\\n\")), mdx(\"h1\", null, \"simpleAtomicIntrinsics\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"simpleAtomicIntrinsics starting...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nProcessing time: 0.564000 (ms)\\nsimpleAtomicIntrinsics completed, returned OK\\n\")), mdx(\"h1\", null, \"simpleAtomicIntrinsics_nvrtc\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"simpleAtomicIntrinsics_nvrtc starting...\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> GPU Device has SM 8.9 compute capability\\nProcessing time: 0.108000 (ms)\\nsimpleAtomicIntrinsics_nvrtc completed, returned OK\\n\")), mdx(\"h1\", null, \"simpleAttributes\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./simpleAttributes Starting...\\n\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nProcessing time: 6674.319824 (ms)\\n\")), mdx(\"h1\", null, \"simpleAWBarrier\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./simpleAWBarrier starting...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nLaunching normVecByDotProductAWBarrier kernel with numBlocks = 120 blockSize = 768\\nResult = PASSED\\n./simpleAWBarrier completed, returned OK\\n\")), mdx(\"h1\", null, \"simpleCallback\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Starting simpleCallback\\nFound 1 CUDA capable GPUs\\nGPU[0] NVIDIA GeForce RTX 4070 Ti supports SM 8.9, capable GPU Callback Functions\\n1 GPUs available to run Callback Functions\\nStarting 8 heterogeneous computing workloads\\nTotal of 8 workloads finished:\\nSuccess\\n\")), mdx(\"h1\", null, \"simpleCooperativeGroups\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"\\nLaunching a single block with 64 threads...\\n\\n Sum of all ranks 0..63 in threadBlockGroup is 2016 (expected 2016)\\n\\n Now creating 4 groups, each of size 16 threads:\\n\\n   Sum of all ranks 0..15 in this tiledPartition16 group is 120 (expected 120)\\n   Sum of all ranks 0..15 in this tiledPartition16 group is 120 (expected 120)\\n   Sum of all ranks 0..15 in this tiledPartition16 group is 120 (expected 120)\\n   Sum of all ranks 0..15 in this tiledPartition16 group is 120 (expected 120)\\n\\n...Done.\\n\\n\")), mdx(\"h1\", null, \"simpleCubemapTexture\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nCUDA device [NVIDIA GeForce RTX 4070 Ti] has 60 Multi-Processors SM 8.9\\nCovering Cubemap data array of 64~3 x 1: Grid size is 8 x 8, each block has 8 x 8 threads\\nProcessing time: 0.005 msec\\n4915.20 Mtexlookups/sec\\nComparing kernel output to expected data\\n\")), mdx(\"h1\", null, \"simpleCudaGraphs\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n16777216 elements\\nthreads per block  = 512\\nGraph Launch iterations = 3\\n\\nNum of nodes in the graph created manually = 7\\n[cudaGraphsManual] Host callback final reduced sum = 0.996214\\n[cudaGraphsManual] Host callback final reduced sum = 0.996214\\n[cudaGraphsManual] Host callback final reduced sum = 0.996214\\nCloned Graph Output.. \\n[cudaGraphsManual] Host callback final reduced sum = 0.996214\\n[cudaGraphsManual] Host callback final reduced sum = 0.996214\\n[cudaGraphsManual] Host callback final reduced sum = 0.996214\\n\\nNum of nodes in the graph created using stream capture API = 7\\n[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214\\n[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214\\n[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214\\nCloned Graph Output.. \\n[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214\\n[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214\\n[cudaGraphsUsingStreamCapture] Host callback final reduced sum = 0.996214\\n\")), mdx(\"h1\", null, \"simpleCUFFT_2d_MGPU\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"\\nPoisson equation using CUFFT library on Multiple GPUs is starting...\\n\\nNo. of GPU on node 1\\nTwo GPUs are required to run simpleCUFFT_2d_MGPU sample code\\n\")), mdx(\"h1\", null, \"simpleDrvRuntime\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"simpleDrvRuntime..\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n> findModulePath found file at <./vectorAdd_kernel64.fatbin>\\n> initCUDA loading module: <./vectorAdd_kernel64.fatbin>\\nResult = PASS\\n\")), mdx(\"h1\", null, \"simpleHyperQ\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"starting hyperQ...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n> Detected Compute SM 8.9 hardware with 60 multi-processors\\nExpected time for serial execution of 32 sets of kernels is between approx. 0.330s and 0.640s\\nExpected time for fully concurrent execution of 32 sets of kernels is approx. 0.020s\\nMeasured time for sample = 0.058s\\n\")), mdx(\"h1\", null, \"simpleIPC\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Process 0: Starting on device 0...\\nStep 0 done\\nProcess 0: verifying...\\nProcess 0 complete!\\n\")), mdx(\"h1\", null, \"simpleLayeredTexture\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[simpleLayeredTexture] - Starting...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nCUDA device [NVIDIA GeForce RTX 4070 Ti] has 60 Multi-Processors SM 8.9\\nCovering 2D data array of 512 x 512: Grid size is 64 x 64, each block has 8 x 8 threads\\nProcessing time: 0.024 msec\\n54613.33 Mtexlookups/sec\\nComparing kernel output to expected data\\n\")), mdx(\"h1\", null, \"simpleMPI\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Invalid MIT-MAGIC-COOKIE-1 keyRunning on 1 nodes\\nAverage of square roots is: 0.667242\\nPASSED\\n\")), mdx(\"h1\", null, \"simpleMultiCopy\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[simpleMultiCopy] - Starting...\\n> Using CUDA device [0]: NVIDIA GeForce RTX 4070 Ti\\n[NVIDIA GeForce RTX 4070 Ti] has 60 MP(s) x 128 (Cores/MP) = 7680 (Cores)\\n> Device name: NVIDIA GeForce RTX 4070 Ti\\n> CUDA Capability 8.9 hardware with 60 multi-processors\\n> scale_factor = 1.00\\n> array_size   = 4194304\\n\\n\\nRelevant properties of this CUDA device\\n(X) Can overlap one CPU<>GPU data transfer with GPU kernel execution (device property \\\"deviceOverlap\\\")\\n(X) Can overlap two CPU<>GPU data transfers with GPU kernel execution\\n    (Compute Capability >= 2.0 AND (Tesla product OR Quadro 4000/5000/6000/K5000)\\n\\nMeasured timings (throughput):\\n Memcpy host to device  : 0.703936 ms (23.833440 GB/s)\\n Memcpy device to host  : 0.640000 ms (26.214401 GB/s)\\n Kernel         : 0.037728 ms (4446.887142 GB/s)\\n\\nTheoretical limits for speedup gained from overlapped data transfers:\\nNo overlap at all (transfer-kernel-transfer): 1.381664 ms \\nCompute can overlap with one transfer: 1.343936 ms\\nCompute can overlap with both data transfers: 0.703936 ms\\n\\nAverage measured timings over 10 repetitions:\\n Avg. time when execution fully serialized  : 1.389363 ms\\n Avg. time when overlapped using 4 streams  : 0.751098 ms\\n Avg. speedup gained (serialized - overlapped)  : 0.638266 ms\\n\\nMeasured throughput:\\n Fully serialized execution     : 24.150944 GB/s\\n Overlapped using 4 streams     : 44.673865 GB/s\\n\")), mdx(\"h1\", null, \"simpleMultiGPU\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Starting simpleMultiGPU\\nCUDA-capable device count: 1\\nGenerating input data...\\n\\nComputing with 1 GPUs...\\n  GPU Processing time: 6.476000 (ms)\\n\\nComputing with Host CPU...\\n\\nComparing GPU and Host CPU results...\\n  GPU sum: 16777296.000000\\n  CPU sum: 16777294.395033\\n  Relative difference: 9.566307E-08 \\n\\n\")), mdx(\"h1\", null, \"simpleOccupancy\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"starting Simple Occupancy\\n\\n[ Manual configuration with 32 threads per block ]\\nPotential occupancy: 50%\\nElapsed time: 0.054464ms\\n\\n[ Automatic, occupancy-based configuration ]\\nSuggested block size: 768\\nMinimum grid size for maximum occupancy: 120\\nPotential occupancy: 100%\\nElapsed time: 0.008512ms\\n\\nTest PASSED\\n\\n\")), mdx(\"h1\", null, \"simpleP2P\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[./simpleP2P] - Starting...\\nChecking for multiple GPUs...\\nCUDA-capable device count: 1\\nTwo or more GPUs with Peer-to-Peer access capability are required for ./simpleP2P.\\nWaiving test.\\n\")), mdx(\"h1\", null, \"simplePitchLinearTexture\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"simplePitchLinearTexture starting...\\n\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n\\nBandwidth (GB/s) for pitch linear: 1.40e+03; for array: 1.83e+03\\n\\nTexture fetch rate (Mpix/s) for pitch linear: 1.75e+05; for array: 2.29e+05\\n\\nsimplePitchLinearTexture completed, returned OK\\n\")), mdx(\"h1\", null, \"simplePrintf\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nDevice 0: \\\"NVIDIA GeForce RTX 4070 Ti\\\" with Compute 8.9 capability\\nprintf() is called. Output:\\n\\n[2, 0]:     Value is:10\\n[2, 1]:     Value is:10\\n[2, 2]:     Value is:10\\n[2, 3]:     Value is:10\\n[2, 4]:     Value is:10\\n[2, 5]:     Value is:10\\n[2, 6]:     Value is:10\\n[2, 7]:     Value is:10\\n[3, 0]:     Value is:10\\n[3, 1]:     Value is:10\\n[3, 2]:     Value is:10\\n[3, 3]:     Value is:10\\n[3, 4]:     Value is:10\\n[3, 5]:     Value is:10\\n[3, 6]:     Value is:10\\n[3, 7]:     Value is:10\\n[1, 0]:     Value is:10\\n[1, 1]:     Value is:10\\n[1, 2]:     Value is:10\\n[1, 3]:     Value is:10\\n[1, 4]:     Value is:10\\n[1, 5]:     Value is:10\\n[1, 6]:     Value is:10\\n[1, 7]:     Value is:10\\n[0, 0]:     Value is:10\\n[0, 1]:     Value is:10\\n[0, 2]:     Value is:10\\n[0, 3]:     Value is:10\\n[0, 4]:     Value is:10\\n[0, 5]:     Value is:10\\n[0, 6]:     Value is:10\\n[0, 7]:     Value is:10\\n\")), mdx(\"h1\", null, \"simpleSeparateCompilation\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"simpleSeparateCompilation starting...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nsimpleSeparateCompilation completed, returned OK\\n\")), mdx(\"h1\", null, \"simpleStreams\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[ simpleStreams ]\\n\\nDevice synchronization method set to = 0 (Automatic Blocking)\\nSetting reps to 100 to demonstrate steady state\\n\\n> GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nDevice: <NVIDIA GeForce RTX 4070 Ti> canMapHostMemory: Yes\\n> CUDA Capable: SM 8.9 hardware\\n> 60 Multiprocessor(s) x 128 (Cores/Multiprocessor) = 7680 (Cores)\\n> scale_factor = 1.0000\\n> array_size   = 16777216\\n\\n> Using CPU/GPU Device Synchronization method (cudaDeviceScheduleAuto)\\n> mmap() allocating 64.00 Mbytes (generic page-aligned system memory)\\n> cudaHostRegister() registering 64.00 Mbytes of generic allocated system memory\\n\\nStarting Test\\nmemcopy:    2.55\\nkernel:     0.31\\nnon-streamed:   2.79\\n4 streams:  2.64\\n-------------------------------\\n\")), mdx(\"h1\", null, \"simpleSurfaceWrite\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"simpleSurfaceWrite starting...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nCUDA device [NVIDIA GeForce RTX 4070 Ti] has 60 Multi-Processors, SM 8.9\\nLoaded 'teapot512.pgm', 512 x 512 pixels\\nProcessing time: 0.007000 (ms)\\n37449.14 Mpixels/sec\\nWrote 'output.pgm'\\nComparing files\\n    output:    <output.pgm>\\n    reference: <../../../../Samples/0_Introduction/simpleSurfaceWrite/data/ref_rotated.pgm>\\nsimpleSurfaceWrite completed, returned OK\\n\")), mdx(\"h1\", null, \"simpleTemplates\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"> runTest<float,32>\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nCUDA device [NVIDIA GeForce RTX 4070 Ti] has 60 Multi-Processors\\nProcessing time: 0.119000 (ms)\\nCompare OK\\n\\n> runTest<int,64>\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nCUDA device [NVIDIA GeForce RTX 4070 Ti] has 60 Multi-Processors\\nProcessing time: 0.043000 (ms)\\nCompare OK\\n\\n\\n[simpleTemplates] -> Test Results: 0 Failures\\n\")), mdx(\"h1\", null, \"simpleTemplates_nvrtc\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"> runTest<float,32>\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> GPU Device has SM 8.9 compute capability\\nProcessing time: 0.064000 (ms)\\nCompare OK\\n\\n> runTest<int,64>\\nProcessing time: 0.050000 (ms)\\nCompare OK\\n\\n\\n[simpleTemplates_nvrtc] -> Test Results: 0 Failures\\n\")), mdx(\"h1\", null, \"simpleTexture\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"simpleTexture starting...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nLoaded 'teapot512.pgm', 512 x 512 pixels\\nProcessing time: 0.007000 (ms)\\n37449.14 Mpixels/sec\\nWrote '../../../../Samples/0_Introduction/simpleTexture/data/teapot512_out.pgm'\\nComparing files\\n    output:    <../../../../Samples/0_Introduction/simpleTexture/data/teapot512_out.pgm>\\n    reference: <../../../../Samples/0_Introduction/simpleTexture/data/ref_rotated.pgm>\\nsimpleTexture completed, returned OK\\n\")), mdx(\"h1\", null, \"simpleTextureDrv\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> GPU Device has SM 8.9 compute capability\\n> findModulePath found file at <./simpleTexture_kernel64.fatbin>\\n> initCUDA loading module: <./simpleTexture_kernel64.fatbin>\\nLoaded 'teapot512.pgm', 512 x 512 pixels\\nProcessing time: 0.007000 (ms)\\n37449.14 Mpixels/sec\\nWrote '../../../../Samples/0_Introduction/simpleTextureDrv/data/teapot512_out.pgm'\\nComparing files\\n    output:    <../../../../Samples/0_Introduction/simpleTextureDrv/data/teapot512_out.pgm>\\n    reference: <../../../../Samples/0_Introduction/simpleTextureDrv/data/ref_rotated.pgm>\\n\")), mdx(\"h1\", null, \"simpleVoteIntrinsics\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[simpleVoteIntrinsics]\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n> GPU device has 60 Multi-Processors, SM 8.9 compute capabilities\\n\\n[VOTE Kernel Test 1/3]\\n    Running <<Vote.Any>> kernel1 ...\\n    OK\\n\\n[VOTE Kernel Test 2/3]\\n    Running <<Vote.All>> kernel2 ...\\n    OK\\n\\n[VOTE Kernel Test 3/3]\\n    Running <<Vote.Any>> kernel3 ...\\n    OK\\n    Shutting down...\\n\")), mdx(\"h1\", null, \"simpleVoteIntrinsics_nvrtc\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> GPU Device has SM 8.9 compute capability\\n[simpleVoteIntrinsics_nvrtc]\\n[VOTE Kernel Test 1/3]\\n    Running <<Vote.Any>> kernel1 ...\\n    OK\\n\\n[VOTE Kernel Test 2/3]\\n    Running <<Vote.All>> kernel2 ...\\n    OK\\n\\n[VOTE Kernel Test 3/3]\\n    Running <<Vote.Any>> kernel3 ...\\n    OK\\n    Shutting down...\\n\")), mdx(\"h1\", null, \"simpleZeroCopy\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"  Device 0: <             Ada >, Compute SM 8.9 detected\\n> Using CUDA Host Allocated (cudaHostAlloc)\\n> vectorAddGPU kernel will add vectors using mapped CPU memory...\\n> Checking the results from vectorAddGPU() ...\\n> Releasing CPU memory...\\n\")), mdx(\"h1\", null, \"SobolQRNG\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Sobol Quasi-Random Number Generator Starting...\\n\\n> number of vectors = 100000\\n> number of dimensions = 100\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nAllocating CPU memory...\\nAllocating GPU memory...\\nInitializing direction numbers...\\nCopying direction numbers to device...\\nExecuting QRNG on GPU...\\nGsamples/s: 52.9101\\nReading results from GPU...\\n\\nExecuting QRNG on CPU...\\nGsamples/s: 0.448853\\nChecking results...\\nL1-Error: 0\\nShutting down...\\n\")), mdx(\"h1\", null, \"stereoDisparity\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[stereoDisparity] Starting...\\n\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\n> GPU device has 60 Multi-Processors, SM 8.9 compute capabilities\\n\\nLoaded <../../../../Samples/5_Domain_Specific/stereoDisparity/data/stereo.im0.640x533.ppm> as image 0\\nLoaded <../../../../Samples/5_Domain_Specific/stereoDisparity/data/stereo.im1.640x533.ppm> as image 1\\nLaunching CUDA stereoDisparityKernel()\\nInput Size  [640x533], Kernel size [17x17], Disparities [-16:0]\\nGPU processing time : 0.1874 (ms)\\nPixel throughput    : 1820.355 Mpixels/sec\\nGPU Checksum = 4293895789, GPU image: <output_GPU.pgm>\\nComputing CPU reference...\\nCPU Checksum = 4293895789, CPU image: <output_CPU.pgm>\\n\")), mdx(\"h1\", null, \"StreamPriorities\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Starting [./StreamPriorities]...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nCUDA stream priority range: LOW: 0 to HIGH: -5\\nelapsed time of kernels launched to LOW priority stream: 2.885 ms\\nelapsed time of kernels launched to HI  priority stream: 1.838 ms\\n\")), mdx(\"h1\", null, \"systemWideAtomics\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nCANNOT access pageable memory\\nsystemWideAtomics completed, returned OK \\n\")), mdx(\"h1\", null, \"template\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"./template Starting...\\n\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nProcessing time: 0.108000 (ms)\\n\")), mdx(\"h1\", null, \"tf32TensorCoreGemm\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Initializing...\\nGPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nM: 8192 (16 x 512)\\nN: 8192 (16 x 512)\\nK: 4096 (8 x 512)\\nPreparing data for GPU...\\nRequired shared memory size: 72 Kb\\nComputing using high performance kernel = 0 - compute_tf32gemm_async_copy\\nTime: 80.129021 ms\\nTFLOPS: 6.86\\n\")), mdx(\"h1\", null, \"topologyQuery\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU0 <-> CPU:\\n  * Atomic Supported: no\\n\")), mdx(\"h1\", null, \"UnifiedMemoryStreams\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nExecuting tasks on host / device\\nTask [2], thread [0] executing on device (368)\\nTask [0], thread [2] executing on device (884)\\nTask [3], thread [1] executing on host (64)\\nTask [1], thread [3] executing on device (387)\\nTask [4], thread [1] executing on device (250)\\nTask [5], thread [0] executing on device (399)\\nTask [6], thread [2] executing on device (131)\\nTask [7], thread [3] executing on device (642)\\nTask [8], thread [1] executing on device (704)\\nTask [9], thread [0] executing on device (469)\\nTask [10], thread [0] executing on device (174)\\nTask [11], thread [1] executing on device (286)\\nTask [12], thread [1] executing on device (513)\\nTask [13], thread [0] executing on device (789)\\nTask [14], thread [0] executing on device (604)\\nTask [15], thread [1] executing on device (133)\\nTask [16], thread [0] executing on device (795)\\nTask [17], thread [3] executing on device (578)\\nTask [18], thread [2] executing on host (91)\\nTask [19], thread [1] executing on device (592)\\nTask [20], thread [1] executing on device (426)\\nTask [21], thread [1] executing on host (64)\\nTask [22], thread [1] executing on device (279)\\nTask [23], thread [0] executing on device (990)\\nTask [24], thread [1] executing on device (160)\\nTask [25], thread [0] executing on device (644)\\nTask [26], thread [0] executing on device (830)\\nTask [27], thread [0] executing on host (64)\\nTask [28], thread [2] executing on device (877)\\nTask [29], thread [2] executing on device (523)\\nTask [30], thread [0] executing on device (834)\\nTask [31], thread [0] executing on device (485)\\nTask [32], thread [2] executing on host (64)\\nTask [33], thread [2] executing on device (577)\\nTask [34], thread [2] executing on device (781)\\nTask [35], thread [2] executing on device (879)\\nTask [36], thread [2] executing on device (564)\\nTask [37], thread [2] executing on device (802)\\nTask [38], thread [2] executing on device (389)\\nTask [39], thread [2] executing on device (954)\\nAll Done!\\n\")), mdx(\"h1\", null, \"vectorAdd\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"[Vector addition of 50000 elements]\\nCopy input data from the host memory to the CUDA device\\nCUDA kernel launch with 196 blocks of 256 threads\\nCopy output data from the CUDA device to the host memory\\nTest PASSED\\nDone\\n\")), mdx(\"h1\", null, \"vectorAddDrv\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Vector Addition (Driver API)\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> findModulePath found file at <./vectorAdd_kernel64.fatbin>\\n> initCUDA loading module: <./vectorAdd_kernel64.fatbin>\\nResult = PASS\\n\")), mdx(\"h1\", null, \"vectorAddMMAP\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Vector Addition (Driver API)\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\nDevice 0 VIRTUAL ADDRESS MANAGEMENT SUPPORTED = 1.\\n> findModulePath found file at <./vectorAdd_kernel64.fatbin>\\n> initCUDA loading module: <./vectorAdd_kernel64.fatbin>\\nResult = PASS\\n\")), mdx(\"h1\", null, \"vectorAdd_nvrtc\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> Using CUDA Device [0]: NVIDIA GeForce RTX 4070 Ti\\n> GPU Device has SM 8.9 compute capability\\n[Vector addition of 50000 elements]\\nCopy input data from the host memory to the CUDA device\\nCUDA kernel launch with 196 blocks of 256 threads\\nCopy output data from the CUDA device to the host memory\\nTest PASSED\\nDone\\n\")), mdx(\"h1\", null, \"warpAggregatedAtomicsCG\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"GPU Device 0: \\\"Ada\\\" with compute capability 8.9\\n\\nCPU max matches GPU max\\n\\nWarp Aggregated Atomics PASSED \\n\")));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"asyncAPI bandwidthTest batchCUBLAS bf16TensorCoreGemm binaryPartitionCG binomialOptions","fields":{"slug":"/2023/11/04/nvidia_4070TI_cuda_report/"},"frontmatter":{"date":"2023-11-04T16:04:00.000Z","title":"Nvidia 4070Ti cuda report","tags":["Nvidia"],"category":"Nvidia"},"tableOfContents":{"items":[{"url":"#asyncapi","title":"asyncAPI"},{"url":"#bandwidthtest","title":"bandwidthTest"},{"url":"#batchcublas","title":"batchCUBLAS"},{"url":"#bf16tensorcoregemm","title":"bf16TensorCoreGemm"},{"url":"#binarypartitioncg","title":"binaryPartitionCG"},{"url":"#binomialoptions","title":"binomialOptions"},{"url":"#binomialoptions_nvrtc","title":"binomialOptions_nvrtc"},{"url":"#blackscholes","title":"BlackScholes"},{"url":"#blackscholes_nvrtc","title":"BlackScholes_nvrtc"},{"url":"#c11_cuda","title":"c++11_cuda"},{"url":"#cdpadvancedquicksort","title":"cdpAdvancedQuicksort"},{"url":"#cdpbeziertessellation","title":"cdpBezierTessellation"},{"url":"#cdpquadtree","title":"cdpQuadtree"},{"url":"#cdpsimpleprint","title":"cdpSimplePrint"},{"url":"#cdpsimplequicksort","title":"cdpSimpleQuicksort"},{"url":"#clock","title":"clock"},{"url":"#clock_nvrtc","title":"clock_nvrtc"},{"url":"#concurrentkernels","title":"concurrentKernels"},{"url":"#conjugategradientmultidevicecg","title":"conjugateGradientMultiDeviceCG"},{"url":"#convolutionfft2d","title":"convolutionFFT2D"},{"url":"#cppintegration","title":"cppIntegration"},{"url":"#cppoverload","title":"cppOverload"},{"url":"#cudacompressiblememory","title":"cudaCompressibleMemory"},{"url":"#cudaopenmp","title":"cudaOpenMP"},{"url":"#cudatensorcoregemm","title":"cudaTensorCoreGemm"},{"url":"#devicequery","title":"deviceQuery"},{"url":"#devicequerydrv","title":"deviceQueryDrv"},{"url":"#dmmatensorcoregemm","title":"dmmaTensorCoreGemm"},{"url":"#dwthaar1d","title":"dwtHaar1D"},{"url":"#dxtc","title":"dxtc"},{"url":"#encode_output","title":"encode_output"},{"url":"#fastwalshtransform","title":"fastWalshTransform"},{"url":"#fdtd3d","title":"FDTD3d"},{"url":"#fp16scalarproduct","title":"fp16ScalarProduct"},{"url":"#globaltoshmemasynccopy","title":"globalToShmemAsyncCopy"},{"url":"#graphmemoryfootprint","title":"graphMemoryFootprint"},{"url":"#graphmemorynodes","title":"graphMemoryNodes"},{"url":"#hsopticalflow","title":"HSOpticalFlow"},{"url":"#immatensorcoregemm","title":"immaTensorCoreGemm"},{"url":"#jacobicudagraphs","title":"jacobiCudaGraphs"},{"url":"#matrixmul","title":"matrixMul"},{"url":"#matrixmuldrv","title":"matrixMulDrv"},{"url":"#matrixmuldynlinkjit","title":"matrixMulDynlinkJIT"},{"url":"#matrixmul_nvrtc","title":"matrixMul_nvrtc"},{"url":"#memmapipcdrv","title":"memMapIPCDrv"},{"url":"#mergesort","title":"mergeSort"},{"url":"#montecarlomultigpu","title":"MonteCarloMultiGPU"},{"url":"#newdelete","title":"newdelete"},{"url":"#nv12tobgrandresize","title":"NV12toBGRandResize"},{"url":"#output","title":"output"},{"url":"#p2pbandwidthlatencytest","title":"p2pBandwidthLatencyTest"},{"url":"#ptxjit","title":"ptxjit"},{"url":"#quasirandomgenerator","title":"quasirandomGenerator"},{"url":"#quasirandomgenerator_nvrtc","title":"quasirandomGenerator_nvrtc"},{"url":"#simpleassert","title":"simpleAssert"},{"url":"#simpleassert_nvrtc","title":"simpleAssert_nvrtc"},{"url":"#simpleatomicintrinsics","title":"simpleAtomicIntrinsics"},{"url":"#simpleatomicintrinsics_nvrtc","title":"simpleAtomicIntrinsics_nvrtc"},{"url":"#simpleattributes","title":"simpleAttributes"},{"url":"#simpleawbarrier","title":"simpleAWBarrier"},{"url":"#simplecallback","title":"simpleCallback"},{"url":"#simplecooperativegroups","title":"simpleCooperativeGroups"},{"url":"#simplecubemaptexture","title":"simpleCubemapTexture"},{"url":"#simplecudagraphs","title":"simpleCudaGraphs"},{"url":"#simplecufft_2d_mgpu","title":"simpleCUFFT_2d_MGPU"},{"url":"#simpledrvruntime","title":"simpleDrvRuntime"},{"url":"#simplehyperq","title":"simpleHyperQ"},{"url":"#simpleipc","title":"simpleIPC"},{"url":"#simplelayeredtexture","title":"simpleLayeredTexture"},{"url":"#simplempi","title":"simpleMPI"},{"url":"#simplemulticopy","title":"simpleMultiCopy"},{"url":"#simplemultigpu","title":"simpleMultiGPU"},{"url":"#simpleoccupancy","title":"simpleOccupancy"},{"url":"#simplep2p","title":"simpleP2P"},{"url":"#simplepitchlineartexture","title":"simplePitchLinearTexture"},{"url":"#simpleprintf","title":"simplePrintf"},{"url":"#simpleseparatecompilation","title":"simpleSeparateCompilation"},{"url":"#simplestreams","title":"simpleStreams"},{"url":"#simplesurfacewrite","title":"simpleSurfaceWrite"},{"url":"#simpletemplates","title":"simpleTemplates"},{"url":"#simpletemplates_nvrtc","title":"simpleTemplates_nvrtc"},{"url":"#simpletexture","title":"simpleTexture"},{"url":"#simpletexturedrv","title":"simpleTextureDrv"},{"url":"#simplevoteintrinsics","title":"simpleVoteIntrinsics"},{"url":"#simplevoteintrinsics_nvrtc","title":"simpleVoteIntrinsics_nvrtc"},{"url":"#simplezerocopy","title":"simpleZeroCopy"},{"url":"#sobolqrng","title":"SobolQRNG"},{"url":"#stereodisparity","title":"stereoDisparity"},{"url":"#streampriorities","title":"StreamPriorities"},{"url":"#systemwideatomics","title":"systemWideAtomics"},{"url":"#template","title":"template"},{"url":"#tf32tensorcoregemm","title":"tf32TensorCoreGemm"},{"url":"#topologyquery","title":"topologyQuery"},{"url":"#unifiedmemorystreams","title":"UnifiedMemoryStreams"},{"url":"#vectoradd","title":"vectorAdd"},{"url":"#vectoradddrv","title":"vectorAddDrv"},{"url":"#vectoraddmmap","title":"vectorAddMMAP"},{"url":"#vectoradd_nvrtc","title":"vectorAdd_nvrtc"},{"url":"#warpaggregatedatomicscg","title":"warpAggregatedAtomicsCG"}]},"timeToRead":1}},"pageContext":{"postID":"00450b1c-295c-5293-a5a6-b0189a05928a","numericId":33,"prevPost":{"slug":"/2023/07/27//","title":""},"nextPost":null,"permalink":"https://www.abellee.cn/2023/11/04/nvidia_4070TI_cuda_report/"}},"staticQueryHashes":["2873555300","3400548236","822196256"]}